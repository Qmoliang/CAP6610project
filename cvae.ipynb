{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ../../MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter-0; Loss: 745.99462890625\n",
      "Iter-1000; Loss: 132.173828125\n",
      "Iter-2000; Loss: 136.5454559326172\n",
      "Iter-3000; Loss: 128.80825805664062\n",
      "Iter-4000; Loss: 115.91888427734375\n",
      "Iter-5000; Loss: 107.06538391113281\n",
      "Iter-6000; Loss: 117.55648040771484\n",
      "Iter-7000; Loss: 106.43943786621094\n",
      "Iter-8000; Loss: 110.27025604248047\n",
      "Iter-9000; Loss: 113.00555419921875\n",
      "Iter-10000; Loss: 106.57183837890625\n",
      "Iter-11000; Loss: 107.37165069580078\n",
      "Iter-12000; Loss: 103.60374450683594\n",
      "Iter-13000; Loss: 106.18244934082031\n",
      "Iter-14000; Loss: 105.93080139160156\n",
      "Iter-15000; Loss: 106.92794036865234\n",
      "Iter-16000; Loss: 106.33598327636719\n",
      "Iter-17000; Loss: 105.53726196289062\n",
      "Iter-18000; Loss: 104.78318786621094\n",
      "Iter-19000; Loss: 109.1433334350586\n",
      "Iter-20000; Loss: 104.49897766113281\n",
      "Iter-21000; Loss: 101.25439453125\n",
      "Iter-22000; Loss: 106.92417907714844\n",
      "Iter-23000; Loss: 96.65645599365234\n",
      "Iter-24000; Loss: 101.31855773925781\n",
      "Iter-25000; Loss: 104.1773452758789\n",
      "Iter-26000; Loss: 102.29130554199219\n",
      "Iter-27000; Loss: 103.88211059570312\n",
      "Iter-28000; Loss: 106.60641479492188\n",
      "Iter-29000; Loss: 104.900146484375\n",
      "Iter-30000; Loss: 99.53402709960938\n",
      "Iter-31000; Loss: 106.09137725830078\n",
      "Iter-32000; Loss: 98.11189270019531\n",
      "Iter-33000; Loss: 101.76429748535156\n",
      "Iter-34000; Loss: 102.62026977539062\n",
      "Iter-35000; Loss: 106.16499328613281\n",
      "Iter-36000; Loss: 97.90513610839844\n",
      "Iter-37000; Loss: 96.21443176269531\n",
      "Iter-38000; Loss: 104.137939453125\n",
      "Iter-39000; Loss: 106.14518737792969\n",
      "Iter-40000; Loss: 104.84661102294922\n",
      "Iter-41000; Loss: 103.50776672363281\n",
      "Iter-42000; Loss: 99.36800384521484\n",
      "Iter-43000; Loss: 101.15316772460938\n",
      "Iter-44000; Loss: 101.76180267333984\n",
      "Iter-45000; Loss: 97.84039306640625\n",
      "Iter-46000; Loss: 102.67727661132812\n",
      "Iter-47000; Loss: 102.90013122558594\n",
      "Iter-48000; Loss: 101.84400939941406\n",
      "Iter-49000; Loss: 102.06387329101562\n",
      "Iter-50000; Loss: 102.441162109375\n",
      "Iter-51000; Loss: 105.05487060546875\n",
      "Iter-52000; Loss: 108.91524505615234\n",
      "Iter-53000; Loss: 101.73589324951172\n",
      "Iter-54000; Loss: 105.4322509765625\n",
      "Iter-55000; Loss: 98.83079528808594\n",
      "Iter-56000; Loss: 100.5272216796875\n",
      "Iter-57000; Loss: 103.81755065917969\n",
      "Iter-58000; Loss: 99.28410339355469\n",
      "Iter-59000; Loss: 103.62052917480469\n",
      "Iter-60000; Loss: 99.94795227050781\n",
      "Iter-61000; Loss: 103.17828369140625\n",
      "Iter-62000; Loss: 103.46803283691406\n",
      "Iter-63000; Loss: 100.37944030761719\n",
      "Iter-64000; Loss: 103.44783020019531\n",
      "Iter-65000; Loss: 102.93492126464844\n",
      "Iter-66000; Loss: 105.5459213256836\n",
      "Iter-67000; Loss: 94.5015869140625\n",
      "Iter-68000; Loss: 103.5833740234375\n",
      "Iter-69000; Loss: 99.78463745117188\n",
      "Iter-70000; Loss: 107.23238372802734\n",
      "Iter-71000; Loss: 98.68080139160156\n",
      "Iter-72000; Loss: 102.06922912597656\n",
      "Iter-73000; Loss: 105.1876449584961\n",
      "Iter-74000; Loss: 105.07810974121094\n",
      "Iter-75000; Loss: 103.62671661376953\n",
      "Iter-76000; Loss: 108.4940414428711\n",
      "Iter-77000; Loss: 102.775390625\n",
      "Iter-78000; Loss: 101.69313049316406\n",
      "Iter-79000; Loss: 99.3534927368164\n",
      "Iter-80000; Loss: 105.62939453125\n",
      "Iter-81000; Loss: 100.82435607910156\n",
      "Iter-82000; Loss: 107.43730163574219\n",
      "Iter-83000; Loss: 101.08901977539062\n",
      "Iter-84000; Loss: 106.60678100585938\n",
      "Iter-85000; Loss: 102.94444274902344\n",
      "Iter-86000; Loss: 98.3096694946289\n",
      "Iter-87000; Loss: 101.72007751464844\n",
      "Iter-88000; Loss: 102.53589630126953\n",
      "Iter-89000; Loss: 104.20932006835938\n",
      "Iter-90000; Loss: 103.22030639648438\n",
      "Iter-91000; Loss: 103.79043579101562\n",
      "Iter-92000; Loss: 99.81346893310547\n",
      "Iter-93000; Loss: 96.62222290039062\n",
      "Iter-94000; Loss: 105.10667419433594\n",
      "Iter-95000; Loss: 101.14169311523438\n",
      "Iter-96000; Loss: 106.16827392578125\n",
      "Iter-97000; Loss: 98.78871154785156\n",
      "Iter-98000; Loss: 102.65576934814453\n",
      "Iter-99000; Loss: 104.47049713134766\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)\n",
    "mb_size = 64\n",
    "Z_dim = 100\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "h_dim = 128\n",
    "cnt = 0\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)\n",
    "\n",
    "\n",
    "# =============================== Q(z|X) ======================================\n",
    "\n",
    "Wxh = xavier_init(size=[X_dim + y_dim, h_dim])\n",
    "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whz_mu = xavier_init(size=[h_dim, Z_dim])\n",
    "bhz_mu = Variable(torch.zeros(Z_dim), requires_grad=True)\n",
    "\n",
    "Whz_var = xavier_init(size=[h_dim, Z_dim])\n",
    "bhz_var = Variable(torch.zeros(Z_dim), requires_grad=True)\n",
    "\n",
    "\n",
    "def Q(X, c):\n",
    "    inputs = torch.cat([X, c], 1)\n",
    "    h = nn.relu(inputs @ Wxh + bxh.repeat(inputs.size(0), 1))\n",
    "    z_mu = h @ Whz_mu + bhz_mu.repeat(h.size(0), 1)\n",
    "    z_var = h @ Whz_var + bhz_var.repeat(h.size(0), 1)\n",
    "    return z_mu, z_var\n",
    "\n",
    "\n",
    "def sample_z(mu, log_var):\n",
    "    eps = Variable(torch.randn(mb_size, Z_dim))\n",
    "    return mu + torch.exp(log_var / 2) * eps\n",
    "\n",
    "\n",
    "# =============================== P(X|z) ======================================\n",
    "\n",
    "Wzh = xavier_init(size=[Z_dim + y_dim, h_dim])\n",
    "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whx = xavier_init(size=[h_dim, X_dim])\n",
    "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
    "\n",
    "\n",
    "def P(z, c):\n",
    "    inputs = torch.cat([z, c], 1)\n",
    "    h = nn.relu(inputs @ Wzh + bzh.repeat(inputs.size(0), 1))\n",
    "    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
    "    return X\n",
    "\n",
    "\n",
    "# =============================== TRAINING ====================================\n",
    "\n",
    "params = [Wxh, bxh, Whz_mu, bhz_mu, Whz_var, bhz_var,\n",
    "          Wzh, bzh, Whx, bhx]\n",
    "\n",
    "solver = optim.Adam(params, lr=lr)\n",
    "\n",
    "for it in range(100000):\n",
    "    X, c = mnist.train.next_batch(mb_size)\n",
    "    X = Variable(torch.from_numpy(X))\n",
    "    c = Variable(torch.from_numpy(c.astype('float32')))\n",
    "\n",
    "    # Forward\n",
    "    z_mu, z_var = Q(X, c)\n",
    "    z = sample_z(z_mu, z_var)\n",
    "    X_sample = P(z, c)\n",
    "\n",
    "    # Loss\n",
    "    recon_loss = nn.binary_cross_entropy(X_sample, X, size_average=False) / mb_size\n",
    "    kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
    "    loss = recon_loss + kl_loss\n",
    "\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    solver.step()\n",
    "\n",
    "    # Housekeeping\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            data = p.grad.data\n",
    "            p.grad = Variable(data.new().resize_as_(data).zero_())\n",
    "\n",
    "    # Print and plot every now and then\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter-{}; Loss: {}'.format(it, loss.data.numpy()))\n",
    "        \n",
    "        c = np.zeros(shape=[mb_size, y_dim], dtype='float32')\n",
    "        c[:, np.random.randint(0, 10)] = 1.\n",
    "        c = Variable(torch.from_numpy(c))\n",
    "        z = Variable(torch.randn(mb_size, Z_dim))\n",
    "        samples = P(z, c).data.numpy()[:16]\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "        if not os.path.exists('out/'):\n",
    "            os.makedirs('out/')\n",
    "\n",
    "        plt.savefig('out/{}.png'.format(str(cnt).zfill(3)), bbox_inches='tight')\n",
    "        cnt += 1\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
